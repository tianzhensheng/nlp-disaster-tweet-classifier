{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2770d-9268-4d40-8b5b-ca28776bea3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8311cf71-56b3-479d-a860-ac2ce698d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 基础路径\n",
    "base_path = Path(r\"C:\\Users\\田\\Desktop\\python实操\\kaggle\\Natural Language Processing with Disaster Tweets\")  # 你的原始路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad225e38-9fba-4ee5-a711-aa6a255336c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(base_path / 'sample_submission.csv')\n",
    "test = pd.read_csv(base_path / 'test.csv')\n",
    "train = pd.read_csv(base_path / 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0480799-2323-4ef8-9245-208d8547d3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae7e6d9-5a29-4a08-91ce-197fa972653b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       0\n",
       "1         2       0\n",
       "2         3       0\n",
       "3         9       0\n",
       "4        11       0\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       0\n",
       "3260  10868       0\n",
       "3261  10874       0\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc8636-6fa3-45b4-b802-9902d410b513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b26c64-01ca-4899-b9a4-b75d40108d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6b8a9c-143b-4e7f-8cb8-56cba1623e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978f0d0b-5874-4baa-ac69-97a84f0fedff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9078abbd-2ba8-49ce-941c-9662de3030f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410eb4d2-c45d-4b2e-9388-faa5707deee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53377fb6-499c-49cd-a5db-10be6d59fdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798da75b-04bd-40a3-a2c7-36bc81c86625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650bcfe8-dcd6-461c-a8ad-4fa3ea6a8002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1c847f-3172-47a3-86c3-487ced5c2ac5",
   "metadata": {},
   "source": [
    "<big>autogluon<big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0128f34c-ed14-4074-bdf3-a0b9c6369a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain_data, xtest_data = train_test_split(train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64661d65-9fbf-4aec-8039-23917a976b79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"./agModels\"\n",
      "Preset alias specified: 'extreme' maps to 'extreme_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       19.02 GB / 31.28 GB (60.8%)\n",
      "Disk Space Avail:   191.01 GB / 452.88 GB (42.2%)\n",
      "===================================================\n",
      "Presets specified: ['extreme']\n",
      "`extreme` preset uses a dynamic portfolio based on dataset size...\n",
      "\tDetected data size: small (<=30000 samples), using `zeroshot_2025_tabfm` portfolio.\n",
      "\t\tNote: `zeroshot_2025_tabfm` portfolio requires a CUDA compatible GPU for best performance.\n",
      "\t\tMake sure you have all the relevant dependencies installed: `pip install autogluon.tabular[tabarena]`.\n",
      "\t\tIt is strongly recommended to use a machine with 64+ GB memory and a CUDA compatible GPU with 32+ GB vRAM when using this preset. \n",
      "\t\tThis portfolio will download foundation model weights from HuggingFace during training. Ensure you have an internet connection or have pre-downloaded the weights to use these models.\n",
      "\t\tThis portfolio was meta-learned with TabArena: https://tabarena.ai\n",
      "Using hyperparameters preset: hyperparameters='zeroshot_2025_tabfm'\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=0, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"C:\\Users\\田\\阿里天池大赛\\agModels\"\n",
      "Train Data Rows:    6090\n",
      "Train Data Columns: 4\n",
      "Label Column:       target\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19326.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.56 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['text']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 442\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 1 | ['id']\n",
      "\t\t('object', [])       : 2 | ['keyword', 'location']\n",
      "\t\t('object', ['text']) : 1 | ['text']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   2 | ['keyword', 'location']\n",
      "\t\t('category', ['text_as_category'])  :   1 | ['text']\n",
      "\t\t('int', [])                         :   1 | ['id']\n",
      "\t\t('int', ['binned', 'text_special']) :  28 | ['text.char_count', 'text.word_count', 'text.capital_ratio', 'text.lower_ratio', 'text.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 436 | ['__nlp__.10', '__nlp__.11', '__nlp__.15', '__nlp__.2015', '__nlp__.30', ...]\n",
      "\t3.6s = Fit runtime\n",
      "\t4 features in original data used to generate 468 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.30 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 5481, Val Rows: 609\n",
      "Large model count detected (21 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'TABPFNV2': [{'ag_args': {'name_suffix': '_r143', 'priority': -1}, 'average_before_softmax': False, 'classification_model_path': 'tabpfn-v2-classifier-od3j1g5m.ckpt', 'inference_config/FINGERPRINT_FEATURE': False, 'inference_config/OUTLIER_REMOVAL_STD': None, 'inference_config/POLYNOMIAL_FEATURES': 'no', 'inference_config/PREPROCESS_TRANSFORMS': [{'append_original': True, 'categorical_name': 'ordinal_very_common_categories_shuffled', 'global_transformer_name': None, 'name': 'safepower', 'subsample_features': -1}, {'append_original': True, 'categorical_name': 'ordinal_very_common_categories_shuffled', 'global_transformer_name': None, 'name': 'quantile_uni', 'subsample_features': -1}], 'inference_config/REGRESSION_Y_PREPROCESS_TRANSFORMS': [None, 'power'], 'inference_config/SUBSAMPLE_SAMPLES': 0.99, 'model_type': 'single', 'n_ensemble_repeats': 4, 'regression_model_path': 'tabpfn-v2-regressor-wyl4o83o.ckpt', 'softmax_temperature': 0.75}, {'ag_args': {'name_suffix': '_r94', 'priority': -3}, 'average_before_softmax': True, 'classification_model_path': 'tabpfn-v2-classifier-vutqq28w.ckpt', 'inference_config/FINGERPRINT_FEATURE': True, 'inference_config/OUTLIER_REMOVAL_STD': None, 'inference_config/POLYNOMIAL_FEATURES': 'no', 'inference_config/PREPROCESS_TRANSFORMS': [{'append_original': True, 'categorical_name': 'ordinal_very_common_categories_shuffled', 'global_transformer_name': None, 'name': 'quantile_uni', 'subsample_features': 0.99}], 'inference_config/REGRESSION_Y_PREPROCESS_TRANSFORMS': [None], 'inference_config/SUBSAMPLE_SAMPLES': None, 'model_type': 'single', 'n_ensemble_repeats': 4, 'regression_model_path': 'tabpfn-v2-regressor-5wof9ojf.ckpt', 'softmax_temperature': 0.9}, {'ag_args': {'name_suffix': '_r181', 'priority': -4}, 'average_before_softmax': False, 'classification_model_path': 'tabpfn-v2-classifier-llderlii.ckpt', 'inference_config/FINGERPRINT_FEATURE': False, 'inference_config/OUTLIER_REMOVAL_STD': 9.0, 'inference_config/POLYNOMIAL_FEATURES': 50, 'inference_config/PREPROCESS_TRANSFORMS': [{'append_original': True, 'categorical_name': 'onehot', 'global_transformer_name': 'svd', 'name': 'quantile_uni_coarse', 'subsample_features': 0.99}], 'inference_config/REGRESSION_Y_PREPROCESS_TRANSFORMS': ['power'], 'inference_config/SUBSAMPLE_SAMPLES': None, 'model_type': 'single', 'n_ensemble_repeats': 4, 'regression_model_path': 'tabpfn-v2-regressor.ckpt', 'softmax_temperature': 0.95}],\n",
      "\t'GBM': [{'ag_args': {'name_suffix': '_r33', 'priority': -2}, 'bagging_fraction': 0.9625293420216, 'bagging_freq': 1, 'cat_l2': 0.1236875455555, 'cat_smooth': 68.8584757332856, 'extra_trees': False, 'feature_fraction': 0.6189215809382, 'lambda_l1': 0.1641757352921, 'lambda_l2': 0.6937755557881, 'learning_rate': 0.0154031028561, 'max_cat_to_onehot': 17, 'min_data_in_leaf': 1, 'min_data_per_group': 30, 'num_leaves': 68}, {'ag_args': {'name_suffix': '_r21', 'priority': -16}, 'bagging_fraction': 0.7218730663234, 'bagging_freq': 1, 'cat_l2': 0.0296205152578, 'cat_smooth': 0.0010255271303, 'extra_trees': False, 'feature_fraction': 0.4557131604374, 'lambda_l1': 0.5219704038237, 'lambda_l2': 0.1070959487853, 'learning_rate': 0.0055891584996, 'max_cat_to_onehot': 71, 'min_data_in_leaf': 50, 'min_data_per_group': 10, 'num_leaves': 30}, {'ag_args': {'name_suffix': '_r11', 'priority': -19}, 'bagging_fraction': 0.775784726514, 'bagging_freq': 1, 'cat_l2': 0.3888471449178, 'cat_smooth': 0.0057144748021, 'extra_trees': True, 'feature_fraction': 0.7732354787904, 'lambda_l1': 0.2211002452568, 'lambda_l2': 1.1318405980187, 'learning_rate': 0.0090151778542, 'max_cat_to_onehot': 15, 'min_data_in_leaf': 4, 'min_data_per_group': 15, 'num_leaves': 2}],\n",
      "\t'CAT': [{'ag_args': {'priority': -5}}, {'ag_args': {'name_suffix': '_r51', 'priority': -10}, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'colsample_bylevel': 0.8771035272558, 'depth': 7, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.0107286863021, 'leaf_estimation_iterations': 2, 'learning_rate': 0.0058424016622, 'max_bin': 254, 'max_ctr_complexity': 4, 'model_size_reg': 0.1307400355809, 'one_hot_max_size': 23, 'subsample': 0.809527841437}, {'ag_args': {'name_suffix': '_r10', 'priority': -12}, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'colsample_bylevel': 0.8994502668431, 'depth': 6, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 1.8187025215896, 'leaf_estimation_iterations': 7, 'learning_rate': 0.005177304142, 'max_bin': 254, 'max_ctr_complexity': 4, 'model_size_reg': 0.5247386875068, 'one_hot_max_size': 53, 'subsample': 0.8705228845742}],\n",
      "\t'TABM': [{'ag_args': {'name_suffix': '_r184', 'priority': -6}, 'amp': False, 'arch_type': 'tabm-mini', 'batch_size': 'auto', 'd_block': 864, 'd_embedding': 24, 'dropout': 0.0, 'gradient_clipping_norm': 1.0, 'lr': 0.0019256819924656217, 'n_blocks': 3, 'num_emb_n_bins': 3, 'num_emb_type': 'pwl', 'patience': 16, 'share_training_batches': False, 'tabm_k': 32, 'weight_decay': 0.0}, {'ag_args': {'name_suffix': '_r69', 'priority': -7}, 'amp': False, 'arch_type': 'tabm-mini', 'batch_size': 'auto', 'd_block': 848, 'd_embedding': 28, 'dropout': 0.40215621636031007, 'gradient_clipping_norm': 1.0, 'lr': 0.0010413640454559532, 'n_blocks': 3, 'num_emb_n_bins': 18, 'num_emb_type': 'pwl', 'patience': 16, 'share_training_batches': False, 'tabm_k': 32, 'weight_decay': 0.0}, {'ag_args': {'name_suffix': '_r52', 'priority': -11}, 'amp': False, 'arch_type': 'tabm-mini', 'batch_size': 'auto', 'd_block': 1024, 'd_embedding': 32, 'dropout': 0.0, 'gradient_clipping_norm': 1.0, 'lr': 0.0006297851297842611, 'n_blocks': 4, 'num_emb_n_bins': 22, 'num_emb_type': 'pwl', 'patience': 16, 'share_training_batches': False, 'tabm_k': 32, 'weight_decay': 0.06900108498839816}],\n",
      "\t'TABICL': [{'ag_args': {'priority': -8}}],\n",
      "\t'XGB': [{'ag_args': {'name_suffix': '_r171', 'priority': -9}, 'colsample_bylevel': 0.9213705632288, 'colsample_bynode': 0.6443385965381, 'enable_categorical': True, 'grow_policy': 'lossguide', 'learning_rate': 0.0068171645251, 'max_cat_to_onehot': 8, 'max_depth': 6, 'max_leaves': 10, 'min_child_weight': 0.0507304250576, 'reg_alpha': 4.2446346389037, 'reg_lambda': 1.4800570021253, 'subsample': 0.9656290596647}, {'ag_args': {'name_suffix': '_r40', 'priority': -18}, 'colsample_bylevel': 0.6377491713202, 'colsample_bynode': 0.9237625621103, 'enable_categorical': True, 'grow_policy': 'lossguide', 'learning_rate': 0.0112462621131, 'max_cat_to_onehot': 33, 'max_depth': 10, 'max_leaves': 35, 'min_child_weight': 0.1403464856034, 'reg_alpha': 3.4960653958503, 'reg_lambda': 1.3062320805235, 'subsample': 0.6948898835178}],\n",
      "\t'MITRA': [{'n_estimators': 1, 'fine_tune': True, 'fine_tune_steps': 50, 'ag.num_gpus': 1, 'ag_args': {'priority': -21}}],\n",
      "}\n",
      "Included models: ['XGB', 'TABM', 'GBM'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_r33 ... Training model for up to 3596.38s of the 3596.38s of remaining time.\n",
      "C:\\anaconda\\envs\\py312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] 系统找不到指定的文件。\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\anaconda\\envs\\py312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\anaconda\\envs\\py312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\anaconda\\envs\\py312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\anaconda\\envs\\py312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\tFitting with cpus=16, gpus=0, mem=0.2/18.4 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.512722\tvalid_set's f1: 0.6875\n",
      "[2000]\tvalid_set's binary_logloss: 0.551765\tvalid_set's f1: 0.701461\n",
      "[3000]\tvalid_set's binary_logloss: 0.592191\tvalid_set's f1: 0.702929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7083\t = Validation score   (f1)\n",
      "\t25.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: TabM_r184 ... Training model for up to 3571.09s of the 3571.09s of remaining time.\n",
      "\tFitting with cpus=16, gpus=1, mem=7.0/18.7 GB\n",
      "\t0.703\t = Validation score   (f1)\n",
      "\t181.37s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: TabM_r69 ... Training model for up to 3389.34s of the 3389.34s of remaining time.\n",
      "\tFitting with cpus=16, gpus=1, mem=8.9/17.9 GB\n",
      "\t0.7056\t = Validation score   (f1)\n",
      "\t198.64s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: XGBoost_r171 ... Training model for up to 3190.30s of the 3190.30s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.6823\t = Validation score   (f1)\n",
      "\t18.85s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: TabM_r52 ... Training model for up to 3171.37s of the 3171.37s of remaining time.\n",
      "\tFitting with cpus=16, gpus=1, mem=10.7/17.6 GB\n",
      "\t0.7075\t = Validation score   (f1)\n",
      "\t325.09s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: TabM ... Training model for up to 2845.82s of the 2845.81s of remaining time.\n",
      "\tFitting with cpus=16, gpus=1, mem=7.8/17.7 GB\n",
      "\t0.7082\t = Validation score   (f1)\n",
      "\t100.53s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: TabM_r191 ... Training model for up to 2745.02s of the 2745.02s of remaining time.\n",
      "\tFitting with cpus=16, gpus=1, mem=5.3/17.2 GB\n",
      "\t0.7183\t = Validation score   (f1)\n",
      "\t166.06s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_r21 ... Training model for up to 2578.72s of the 2578.72s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/15.6 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.514794\tvalid_set's f1: 0.698152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7037\t = Validation score   (f1)\n",
      "\t4.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost_r40 ... Training model for up to 2574.45s of the 2574.45s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.6908\t = Validation score   (f1)\n",
      "\t8.6s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_r11 ... Training model for up to 2565.80s of the 2565.80s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/15.7 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.548674\tvalid_set's f1: 0.578704\n",
      "[2000]\tvalid_set's binary_logloss: 0.531804\tvalid_set's f1: 0.659529\n",
      "[3000]\tvalid_set's binary_logloss: 0.527415\tvalid_set's f1: 0.705155\n",
      "[4000]\tvalid_set's binary_logloss: 0.524514\tvalid_set's f1: 0.712551\n",
      "[5000]\tvalid_set's binary_logloss: 0.522447\tvalid_set's f1: 0.704453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.714\t = Validation score   (f1)\n",
      "\t10.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: TabM_r49 ... Training model for up to 2555.38s of the 2555.37s of remaining time.\n",
      "\tFitting with cpus=16, gpus=1, mem=10.6/15.6 GB\n",
      "\t0.7069\t = Validation score   (f1)\n",
      "\t122.55s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 2432.36s of remaining time.\n",
      "\tEnsemble Weights: {'TabM_r191': 0.6, 'LightGBM_r11': 0.4}\n",
      "\t0.7214\t = Validation score   (f1)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1167.84s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2708.0 rows/s (609 batch size)\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.7214\n",
      "\tBest Threshold: 0.380\t| val: 0.7376\n",
      "Updating predictor.decision_threshold from 0.5 -> 0.38\n",
      "\tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\tPrediction probabilities of the positive class >0.38 will be predicted as the positive class (1). This can significantly impact metric scores.\n",
      "\tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\田\\阿里天池大赛\\agModels\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 20min 54s\n",
      "Wall time: 19min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x1ad4dddb620>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "# 正确的预设名（使用你的版本支持的）\n",
    "predictor = TabularPredictor(\n",
    "    label='target',  # 替换为你的标签列名\n",
    "    path='./agModels',\n",
    "    # verbosity=1,#0,1,2,3,4,    \n",
    "    eval_metric='f1',     # 'balanced_accuracy',mae,f1_macro,roc_auc,    \n",
    "    problem_type='binary',    # 'multiclass',binary,regression,    \n",
    ")\n",
    "\n",
    "# 关键：使用 'good_quality' 但关闭所有高级功能\n",
    "predictor.fit(\n",
    "    train_data=xtrain_data,    \n",
    "    presets='best', #  ,extreme，best，high，good，medium,\n",
    "    \n",
    "    num_bag_folds=0,      # 关闭bagging\n",
    "    num_stack_levels=0,   # 关闭stacking    \n",
    "    # auto_stack = True    ,  # 自动堆叠\n",
    "    \n",
    "    \n",
    "    included_model_types=['XGB',],  \n",
    "    # time_limit=100,  # 先试5分钟   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d95a2-5d74-431c-afdb-020bc7636469",
   "metadata": {},
   "outputs": [],
   "source": [
    "'XGB','GBM','RF','XT','FASTAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c1762-5173-40e4-adc1-3a8bb0ddc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "'TABM','GBM','XGB',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ef850-5bf4-4ece-898a-c584189cf91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d695481c-61f2-4b2d-bcb7-60175256ec57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.721443</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.224890</td>\n",
       "      <td>176.440517</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.139832</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TabM_r191</td>\n",
       "      <td>0.718254</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.207896</td>\n",
       "      <td>166.057215</td>\n",
       "      <td>0.207896</td>\n",
       "      <td>166.057215</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM_r11</td>\n",
       "      <td>0.713996</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.015002</td>\n",
       "      <td>10.243470</td>\n",
       "      <td>0.015002</td>\n",
       "      <td>10.243470</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM_r33</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>25.087075</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>25.087075</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TabM</td>\n",
       "      <td>0.708249</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.209465</td>\n",
       "      <td>100.532593</td>\n",
       "      <td>0.209465</td>\n",
       "      <td>100.532593</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TabM_r52</td>\n",
       "      <td>0.707510</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.321146</td>\n",
       "      <td>325.090560</td>\n",
       "      <td>0.321146</td>\n",
       "      <td>325.090560</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TabM_r49</td>\n",
       "      <td>0.706861</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.254720</td>\n",
       "      <td>122.549969</td>\n",
       "      <td>0.254720</td>\n",
       "      <td>122.549969</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TabM_r69</td>\n",
       "      <td>0.705637</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.284041</td>\n",
       "      <td>198.637933</td>\n",
       "      <td>0.284041</td>\n",
       "      <td>198.637933</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LightGBM_r21</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>4.212158</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>4.212158</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TabM_r184</td>\n",
       "      <td>0.703030</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.282282</td>\n",
       "      <td>181.371925</td>\n",
       "      <td>0.282282</td>\n",
       "      <td>181.371925</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost_r40</td>\n",
       "      <td>0.690832</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.038004</td>\n",
       "      <td>8.596123</td>\n",
       "      <td>0.038004</td>\n",
       "      <td>8.596123</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost_r171</td>\n",
       "      <td>0.682303</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.043555</td>\n",
       "      <td>18.846883</td>\n",
       "      <td>0.043555</td>\n",
       "      <td>18.846883</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  score_val eval_metric  pred_time_val    fit_time  \\\n",
       "0   WeightedEnsemble_L2   0.721443          f1       0.224890  176.440517   \n",
       "1             TabM_r191   0.718254          f1       0.207896  166.057215   \n",
       "2          LightGBM_r11   0.713996          f1       0.015002   10.243470   \n",
       "3          LightGBM_r33   0.708333          f1       0.021110   25.087075   \n",
       "4                  TabM   0.708249          f1       0.209465  100.532593   \n",
       "5              TabM_r52   0.707510          f1       0.321146  325.090560   \n",
       "6              TabM_r49   0.706861          f1       0.254720  122.549969   \n",
       "7              TabM_r69   0.705637          f1       0.284041  198.637933   \n",
       "8          LightGBM_r21   0.703704          f1       0.012002    4.212158   \n",
       "9             TabM_r184   0.703030          f1       0.282282  181.371925   \n",
       "10          XGBoost_r40   0.690832          f1       0.038004    8.596123   \n",
       "11         XGBoost_r171   0.682303          f1       0.043555   18.846883   \n",
       "\n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                 0.001993           0.139832            2       True   \n",
       "1                 0.207896         166.057215            1       True   \n",
       "2                 0.015002          10.243470            1       True   \n",
       "3                 0.021110          25.087075            1       True   \n",
       "4                 0.209465         100.532593            1       True   \n",
       "5                 0.321146         325.090560            1       True   \n",
       "6                 0.254720         122.549969            1       True   \n",
       "7                 0.284041         198.637933            1       True   \n",
       "8                 0.012002           4.212158            1       True   \n",
       "9                 0.282282         181.371925            1       True   \n",
       "10                0.038004           8.596123            1       True   \n",
       "11                0.043555          18.846883            1       True   \n",
       "\n",
       "    fit_order  \n",
       "0          12  \n",
       "1           7  \n",
       "2          10  \n",
       "3           1  \n",
       "4           6  \n",
       "5           5  \n",
       "6          11  \n",
       "7           3  \n",
       "8           8  \n",
       "9           2  \n",
       "10          9  \n",
       "11          4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f870a2-a1f7-4a4d-b6a1-3557e16eb3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.7403708987161198,\n",
       " 'accuracy': 0.7609980302035456,\n",
       " 'balanced_accuracy': 0.7659786399071975,\n",
       " 'mcc': 0.5261529026122175,\n",
       " 'roc_auc': 0.8451569568390729,\n",
       " 'precision': 0.6892430278884463,\n",
       " 'recall': 0.7996918335901386}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(xtest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7053128-3f0f-4e39-9148-dffbcd14fa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 4 features using 1523 rows with 5 shuffle sets...\n",
      "\t19.47s\t= Expected runtime (3.89s per shuffle set)\n",
      "\t8.0s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 38.3 s\n",
      "Wall time: 8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.203570</td>\n",
       "      <td>0.005707</td>\n",
       "      <td>7.401976e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.215320</td>\n",
       "      <td>0.191821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>0.102147</td>\n",
       "      <td>0.011388</td>\n",
       "      <td>1.823644e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.125596</td>\n",
       "      <td>0.078699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>0.006447</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>1.181500e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014790</td>\n",
       "      <td>-0.001896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>4.794186e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          importance    stddev       p_value  n  p99_high   p99_low\n",
       "text        0.203570  0.005707  7.401976e-08  5  0.215320  0.191821\n",
       "keyword     0.102147  0.011388  1.823644e-05  5  0.125596  0.078699\n",
       "location    0.006447  0.004052  1.181500e-02  5  0.014790 -0.001896\n",
       "id          0.002192  0.001052  4.794186e-03  5  0.004358  0.000026"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictor.feature_importance(xtest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3e9b2-da58-498d-9450-51da70f1f0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8effb33a-2e63-432e-8540-c06c713f10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cccd6fa2-1876-4736-9d13-e778d7bb7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=test[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed261f3-e5a3-4cca-b16c-64ab6e7a1441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fa4843349349f98b303f4050de6c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\py312\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:434: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = predictor.predict(test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56a74459-d88d-49f5-beb4-53c1dd3b76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=test[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cfeecce-5378-4129-9503-a0729f25523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target']=predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1d37597-dcca-43bf-868b-39ece9fae058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       0\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faaabfcb-2808-478b-9e68-45dc41d0bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('kaggel_nlp_getting_started-0106-05.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ab73f-71ea-4b9a-ae57-fd27cbe7af79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e04867-804e-4190-949d-8ff98cf13369",
   "metadata": {},
   "outputs": [],
   "source": [
    "bset:0.75543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debfbed-0c57-48d5-8bf2-ecdb839c6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extream:0.76861,19min 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd5ad7-99de-4703-a396-f2a609b31b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "多模态：0.77965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b79ba-9c2f-4f95-9ae0-d62cac55932d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63220acb-de71-4281-b655-3e1439b105a7",
   "metadata": {},
   "source": [
    "<big>传统ML，counter，TF-IDF，文本向量化<big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e3b6dd7-30f6-4da1-87fb-4bc31338a042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 数据预处理...\n",
      "训练集: 7613条, 测试集: 3263条\n",
      "\n",
      "==================================================\n",
      "方案1: CountVectorizer + 逻辑回归\n",
      "==================================================\n",
      "✅ 完成\n",
      "\n",
      "==================================================\n",
      "方案2: TF-IDF + 逻辑回归\n",
      "==================================================\n",
      "✅ 完成\n",
      "\n",
      "==================================================\n",
      "方案3: TF-IDF + 随机森林\n",
      "==================================================\n",
      "✅ 完成\n",
      "\n",
      "==================================================\n",
      "方案4: BERT向量化 + 逻辑回归\n",
      "==================================================\n",
      "加载BERT模型...\n",
      "提取BERT向量（简化版）...\n",
      "✅ 完成\n",
      "\n",
      "==================================================\n",
      "6. 生成预测结果\n",
      "==================================================\n",
      "四种结果已保存:\n",
      "CPU times: total: 9.33 s\n",
      "Wall time: 8.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ==============================\n",
    "# 灾难推文分类 - 四种方案对比\n",
    "# 取消交叉验证，直接训练\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================\n",
    "# 1. 数据预处理\n",
    "# ==============================\n",
    "print(\"1. 数据预处理...\")\n",
    "\n",
    "def clean_text(tweet):\n",
    "    tweet = str(tweet)\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet.lower()\n",
    "\n",
    "df_train = train\n",
    "df_test = test\n",
    "\n",
    "df_train['text_cleaned'] = df_train['text'].apply(clean_text)\n",
    "df_test['text_cleaned'] = df_test['text'].apply(clean_text)\n",
    "\n",
    "X = df_train['text_cleaned'].values\n",
    "y = df_train['target'].values\n",
    "X_test = df_test['text_cleaned'].values\n",
    "\n",
    "print(f\"训练集: {len(X)}条, 测试集: {len(X_test)}条\")\n",
    "\n",
    "# ==============================\n",
    "# 2. 方案1: CountVectorizer\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"方案1: CountVectorizer + 逻辑回归\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=3000, stop_words='english')\n",
    "X_count = count_vectorizer.fit_transform(X)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "count_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "count_model.fit(X_count, y)\n",
    "y_pred_count = count_model.predict(X_test_count)\n",
    "\n",
    "print(\"✅ 完成\")\n",
    "\n",
    "# ==============================\n",
    "# 3. 方案2: TF-IDF\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"方案2: TF-IDF + 逻辑回归\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "tfidf_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "tfidf_model.fit(X_tfidf, y)\n",
    "y_pred_tfidf = tfidf_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"✅ 完成\")\n",
    "\n",
    "# ==============================\n",
    "# 4. 方案3: 随机森林\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"方案3: TF-IDF + 随机森林\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=50, max_depth=15, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_tfidf, y)\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"✅ 完成\")\n",
    "\n",
    "# ==============================\n",
    "# 5. 方案4: BERT向量化\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"方案4: BERT向量化 + 逻辑回归\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"加载BERT模型...\")\n",
    "MODEL_NAME = r'C:\\Users\\田\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "# 简化BERT向量提取\n",
    "def get_bert_vectors(texts, batch_size=16):\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch.tolist(), padding=True, truncation=True, max_length=64, return_tensors='pt')\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        \n",
    "        vectors.append(outputs.last_hidden_state[:, 0, :].cpu().numpy())\n",
    "        \n",
    "    return np.vstack(vectors)\n",
    "\n",
    "# 只处理前2000条，加快速度\n",
    "print(\"提取BERT向量（简化版）...\")\n",
    "sample_size = min(2000, len(X), len(X_test))\n",
    "X_bert_sample = get_bert_vectors(X[:sample_size])\n",
    "X_test_bert_sample = get_bert_vectors(X_test[:sample_size])\n",
    "\n",
    "# 训练\n",
    "bert_model_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "bert_model_lr.fit(X_bert_sample, y[:sample_size])\n",
    "\n",
    "# 预测\n",
    "y_pred_bert = np.zeros(len(X_test))\n",
    "y_pred_bert[:sample_size] = bert_model_lr.predict(X_test_bert_sample)\n",
    "y_pred_bert[sample_size:] = 0  # 其他用0填充\n",
    "\n",
    "print(\"✅ 完成\")\n",
    "\n",
    "# ==============================\n",
    "# 6. 生成四种预测结果\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"6. 生成预测结果\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 保存每种方法的预测\n",
    "submissions = {\n",
    "    'count': sample_submission.copy(),\n",
    "    'tfidf': sample_submission.copy(),\n",
    "    'rf': sample_submission.copy(),\n",
    "    'bert': sample_submission.copy()\n",
    "}\n",
    "\n",
    "# 确保长度一致\n",
    "for i, (key, pred) in enumerate([('count', y_pred_count), ('tfidf', y_pred_tfidf), \n",
    "                                  ('rf', y_pred_rf), ('bert', y_pred_bert)]):\n",
    "    if len(pred) > len(submissions[key]):\n",
    "        submissions[key]['target'] = pred[:len(submissions[key])]\n",
    "    else:\n",
    "        # 补充0\n",
    "        padded = np.pad(pred, (0, len(submissions[key])-len(pred)), 'constant')\n",
    "        submissions[key]['target'] = padded\n",
    "\n",
    "# 保存四种结果\n",
    "# submissions['count'].to_csv('count_submission.csv', index=False)\n",
    "# submissions['tfidf'].to_csv('tfidf_submission.csv', index=False)\n",
    "# submissions['rf'].to_csv('rf_submission.csv', index=False)\n",
    "# submissions['bert'].to_csv('bert_submission.csv', index=False)\n",
    "\n",
    "print(\"四种结果已保存:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5a9a0-22f6-476b-99e8-cce032496de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03cba64-64ce-4588-83a2-1001e3eff7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ecd446-efd0-475f-9d4f-528a95d6d81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71dab050-112a-4b35-bea2-9f27a249d4ce",
   "metadata": {},
   "source": [
    "<big>sft<big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5094f794-d83d-4cfa-9ed7-32d4d39c07ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\田\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始灾难推文分类训练\n",
      "设备: cuda\n",
      "模型: C:\\Users\\田\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased\n",
      "\n",
      "📊 加载和预处理数据...\n",
      "\n",
      "🔄 加载tokenizer和模型\n",
      "✅ 模型加载完成\n",
      "\n",
      "🎯 开始训练，共 2 个epoch\n",
      "\n",
      "==================================================\n",
      "Epoch 1/2\n",
      "==================================================\n",
      "  Batch 10/191, Loss: 0.6845\n",
      "  Batch 20/191, Loss: 0.6714\n",
      "  Batch 30/191, Loss: 0.6396\n",
      "  Batch 40/191, Loss: 0.6000\n",
      "  Batch 50/191, Loss: 0.5700\n",
      "  Batch 60/191, Loss: 0.5487\n",
      "  Batch 70/191, Loss: 0.5285\n",
      "  Batch 80/191, Loss: 0.5154\n",
      "  Batch 90/191, Loss: 0.5154\n",
      "  Batch 100/191, Loss: 0.5096\n",
      "  Batch 110/191, Loss: 0.5018\n",
      "  Batch 120/191, Loss: 0.4936\n",
      "  Batch 130/191, Loss: 0.4858\n",
      "  Batch 140/191, Loss: 0.4796\n",
      "  Batch 150/191, Loss: 0.4766\n",
      "  Batch 160/191, Loss: 0.4713\n",
      "  Batch 170/191, Loss: 0.4677\n",
      "  Batch 180/191, Loss: 0.4664\n",
      "  Batch 190/191, Loss: 0.4636\n",
      "训练损失: 0.4643\n",
      "验证集F1分数: 0.8393\n",
      "✅ 保存最佳模型，F1: 0.8393\n",
      "\n",
      "==================================================\n",
      "Epoch 2/2\n",
      "==================================================\n",
      "  Batch 10/191, Loss: 0.3331\n",
      "  Batch 20/191, Loss: 0.3515\n",
      "  Batch 30/191, Loss: 0.3366\n",
      "  Batch 40/191, Loss: 0.3315\n",
      "  Batch 50/191, Loss: 0.3362\n",
      "  Batch 60/191, Loss: 0.3342\n",
      "  Batch 70/191, Loss: 0.3372\n",
      "  Batch 80/191, Loss: 0.3383\n",
      "  Batch 90/191, Loss: 0.3330\n",
      "  Batch 100/191, Loss: 0.3419\n",
      "  Batch 110/191, Loss: 0.3406\n",
      "  Batch 120/191, Loss: 0.3396\n",
      "  Batch 130/191, Loss: 0.3391\n",
      "  Batch 140/191, Loss: 0.3385\n",
      "  Batch 150/191, Loss: 0.3416\n",
      "  Batch 160/191, Loss: 0.3410\n",
      "  Batch 170/191, Loss: 0.3414\n",
      "  Batch 180/191, Loss: 0.3422\n",
      "  Batch 190/191, Loss: 0.3411\n",
      "训练损失: 0.3415\n",
      "验证集F1分数: 0.8461\n",
      "✅ 保存最佳模型，F1: 0.8461\n",
      "\n",
      "🔮 在测试集上生成预测...\n",
      "\n",
      "💾 生成提交文件...\n",
      "✅ 提交文件已保存: bert_sft_submission.csv\n",
      "📈 最佳F1分数: 0.8461\n",
      "\n",
      "✅ 训练完成!\n",
      "\n",
      "🔍 验证预测结果:\n",
      "测试集大小: 3263\n",
      "预测分布: 0=1989, 1=1274\n",
      "CPU times: total: 3min 36s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ==============================\n",
    "# 灾难推文分类 - SFT风格BERT\n",
    "# 保持BERT模型，但用生成式训练方式\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,  # 用回分类模型\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================\n",
    "# 1. 配置参数\n",
    "# ==============================\n",
    "SEED = 42\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LR = 2e-5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 使用BERT模型\n",
    "MODEL_NAME = r'C:\\Users\\田\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased'\n",
    "\n",
    "# ==============================\n",
    "# 2. 数据处理函数\n",
    "# ==============================\n",
    "def clean_text(tweet):\n",
    "    \"\"\"清理文本\"\"\"\n",
    "    tweet = str(tweet)\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "def format_sft_prompt(text, label=None):\n",
    "    \"\"\"\n",
    "    格式化指令提示 - 为BERT调整\n",
    "    \"\"\"\n",
    "    instruction = \"判断以下推文是否描述真实灾难事件：\"\n",
    "    prompt = f\"{instruction} {text}\"\n",
    "    return prompt\n",
    "\n",
    "# ==============================\n",
    "# 3. 数据集类\n",
    "# ==============================\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = format_sft_prompt(str(self.texts[idx]))\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        else:\n",
    "            # 测试数据没有标签\n",
    "            item['labels'] = torch.tensor(0, dtype=torch.long)  # 占位符\n",
    "            \n",
    "        return item\n",
    "\n",
    "# ==============================\n",
    "# 4. 训练和评估函数\n",
    "# ==============================\n",
    "def train_epoch(model, dataloader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            print(f\"  Batch {batch_idx+1}/{len(dataloader)}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "    return f1, all_predictions\n",
    "\n",
    "# ==============================\n",
    "# 5. 主训练流程\n",
    "# ==============================\n",
    "def main():\n",
    "    print(\"🚀 开始灾难推文分类训练\")\n",
    "    print(f\"设备: {DEVICE}\")\n",
    "    print(f\"模型: {MODEL_NAME}\")\n",
    "    \n",
    "    # 加载数据\n",
    "    print(\"\\n📊 加载和预处理数据...\")\n",
    "    df_train = train\n",
    "    df_test = test\n",
    "    \n",
    "    # 清理文本\n",
    "    df_train['text_cleaned'] = df_train['text'].apply(clean_text)\n",
    "    df_test['text_cleaned'] = df_test['text'].apply(clean_text)\n",
    "    \n",
    "    # 初始化tokenizer和模型\n",
    "    print(f\"\\n🔄 加载tokenizer和模型\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # 关键修复：为BERT设置padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    # 加载模型\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    print(f\"✅ 模型加载完成\")\n",
    "    \n",
    "    # 准备数据\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df_train['text_cleaned'].values,\n",
    "        df_train['target'].values,\n",
    "        test_size=0.2,\n",
    "        random_state=SEED,\n",
    "        stratify=df_train['target']\n",
    "    )\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = TweetDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = TweetDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "    test_dataset = TweetDataset(df_test['text_cleaned'].values, None, tokenizer, MAX_LEN)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(total_steps * 0.1),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    print(f\"\\n🎯 开始训练，共 {EPOCHS} 个epoch\")\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 训练\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "        print(f\"训练损失: {train_loss:.4f}\")\n",
    "        \n",
    "        # 评估\n",
    "        f1_score_val, _ = evaluate_model(model, val_loader)\n",
    "        print(f\"验证集F1分数: {f1_score_val:.4f}\")\n",
    "        \n",
    "        if f1_score_val > best_f1:\n",
    "            best_f1 = f1_score_val\n",
    "            torch.save(model.state_dict(), f\"best_model_epoch{epoch+1}.pth\")\n",
    "            print(f\"✅ 保存最佳模型，F1: {best_f1:.4f}\")\n",
    "    \n",
    "    # 测试集预测\n",
    "    print(f\"\\n🔮 在测试集上生成预测...\")\n",
    "    if EPOCHS > 0:\n",
    "        model.load_state_dict(torch.load(f\"best_model_epoch{EPOCHS}.pth\"))\n",
    "    \n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_predictions.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    # 生成提交文件\n",
    "    print(f\"\\n💾 生成提交文件...\")\n",
    "    submission = sample_submission.copy()\n",
    "    submission['target'] = test_predictions[:len(submission)]\n",
    "    \n",
    "    submission.to_csv('bert_sft_submission.csv', index=False)\n",
    "    print(f\"✅ 提交文件已保存: bert_sft_submission.csv\")\n",
    "    print(f\"📈 最佳F1分数: {best_f1:.4f}\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# ==============================\n",
    "# 6. 运行训练\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        submission = main()\n",
    "        print(\"\\n✅ 训练完成!\")\n",
    "        \n",
    "        # 验证前几个预测\n",
    "        print(\"\\n🔍 验证预测结果:\")\n",
    "        print(f\"测试集大小: {len(submission)}\")\n",
    "        print(f\"预测分布: 0={sum(submission['target']==0)}, 1={sum(submission['target']==1)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6186c73-7e97-46ef-9376-1b69cae03f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Score: 0.83205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cc660-ddf3-4424-b62a-3a76d4bef25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6ff7535-c346-47c9-b97b-adf0a6ef13ad",
   "metadata": {},
   "source": [
    "<big>transformer，all<big>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1346d07-5372-4551-9a5b-966c1155367b",
   "metadata": {},
   "source": [
    "<big>风控相关<big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27869adc-2f90-497c-bb8a-ce52bfc99f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Cleaning text...\n",
      "\n",
      "Initializing tokenizer and model...\n",
      "✓ Tokenizer loaded: BertTokenizerFast\n",
      "\n",
      "==================== Fold 1/2 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\田\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/2\n",
      "调试信息 - 输入数据类型:\n",
      "input_ids: torch.int64, shape: torch.Size([32, 128])\n",
      "attention_mask: torch.int64, shape: torch.Size([32, 128])\n",
      "labels: torch.int64, shape: torch.Size([32])\n",
      "    Train Loss: 0.4774, Val F1: 0.8189\n",
      "  Epoch 2/2\n",
      "    Train Loss: 0.3338, Val F1: 0.8278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\田\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1 completed. Best Val F1: 0.8278\n",
      "\n",
      "==================== Fold 2/2 ====================\n",
      "  Epoch 1/2\n",
      "    Train Loss: 0.4584, Val F1: 0.8085\n",
      "  Epoch 2/2\n",
      "    Train Loss: 0.3422, Val F1: 0.8345\n",
      "  Fold 2 completed. Best Val F1: 0.8345\n",
      "\n",
      "Generating submission...\n",
      "Submission generated successfully!\n",
      "CPU times: total: 5min 38s\n",
      "Wall time: 5min 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\anaconda\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<timed exec>:216: RuntimeWarning: invalid value encountered in cast\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ==============================\n",
    "# 灾难推文分类\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 1. 配置\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "K = 2\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LR = 2e-5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "def clean(tweet):\n",
    "    tweet = str(tweet)\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "# ------------------------------\n",
    "# 2. 数据加载\n",
    "# ------------------------------\n",
    "print(\"Loading data...\")\n",
    "df_train = train\n",
    "df_test = test\n",
    "\n",
    "print(\"Cleaning text...\")\n",
    "df_train['text_cleaned'] = df_train['text'].apply(clean)\n",
    "df_test['text_cleaned'] = df_test['text'].apply(clean)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Dataset类\n",
    "# ------------------------------\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten().long(),  # 添加.long()\n",
    "            'attention_mask': encoding['attention_mask'].flatten().long(),  # 添加.long()\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ------------------------------\n",
    "# 4. 训练函数\n",
    "# ------------------------------\n",
    "def train_model(model, dataloader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 关键修改：确保数据类型正确\n",
    "        input_ids = batch['input_ids'].to(DEVICE).long()  # 添加.long()\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE).long()  # 添加.long()\n",
    "        labels = batch['label'].to(DEVICE).long()  # 添加.long()\n",
    "        \n",
    "        # 调试：打印数据类型\n",
    "        if hasattr(train_model, 'debug_printed') is False:\n",
    "            print(f\"调试信息 - 输入数据类型:\")\n",
    "            print(f\"input_ids: {input_ids.dtype}, shape: {input_ids.shape}\")\n",
    "            print(f\"attention_mask: {attention_mask.dtype}, shape: {attention_mask.shape}\")\n",
    "            print(f\"labels: {labels.dtype}, shape: {labels.shape}\")\n",
    "            train_model.debug_printed = True\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # 同样确保数据类型\n",
    "            input_ids = batch['input_ids'].to(DEVICE).long()\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE).long()\n",
    "            labels = batch['label'].to(DEVICE).long()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "    return f1_score(targets, preds, average='macro')\n",
    "\n",
    "# ------------------------------\n",
    "# 5. 主训练流程\n",
    "# ------------------------------\n",
    "print(\"\\nInitializing tokenizer and model...\")\n",
    "\n",
    "# 使用标准BERT模型\n",
    "model_name = r'C:\\Users\\田\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased'\n",
    "\n",
    "\n",
    "# model_path = r'C:\\Users\\田\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"✓ Tokenizer loaded: {type(tokenizer).__name__}\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)\n",
    "all_test_preds = []\n",
    "\n",
    "all_test_logits = []  # 新增：用于存储每个 fold 的 logits，风控修改\n",
    "\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n",
    "    print(f\"\\n{'='*20} Fold {fold + 1}/{K} {'='*20}\")\n",
    "    \n",
    "    # 准备数据\n",
    "    train_texts = df_train.iloc[trn_idx]['text_cleaned'].values\n",
    "    train_labels = df_train.iloc[trn_idx]['target'].values\n",
    "    val_texts = df_train.iloc[val_idx]['text_cleaned'].values\n",
    "    val_labels = df_train.iloc[val_idx]['target'].values\n",
    "    \n",
    "    train_dataset = TweetDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = TweetDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 加载模型\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=LR, eps=1e-8)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    # 训练循环\n",
    "    best_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"  Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        train_loss = train_model(model, train_loader, optimizer, scheduler)\n",
    "        val_f1 = eval_model(model, val_loader)\n",
    "        print(f\"    Train Loss: {train_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "    \n",
    "    # 预测测试集\n",
    "    test_dataset = TweetDataset(df_test['text_cleaned'].values, [0]*len(df_test), tokenizer, MAX_LEN)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    #风控修改\n",
    "    fold_logits = []  # 改为保存 logits\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE).long()\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE).long()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            fold_logits.append(logits.cpu().numpy())  # 保存原始 logits\n",
    "    \n",
    "    # 合并所有 batch 的 logits\n",
    "    fold_logits = np.vstack(fold_logits)  # shape: [n_test, 2]\n",
    "    all_test_logits.append(fold_logits)\n",
    "\n",
    "    print(f\"  Fold {fold + 1} completed. Best Val F1: {best_f1:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. 生成提交文件\n",
    "# ------------------------------\n",
    "print(\"\\nGenerating submission...\")\n",
    "final_preds = np.round(np.mean(all_test_preds, axis=0)).astype(int)\n",
    "submission = sample_submission.copy()\n",
    "submission['target'] = final_preds\n",
    "print(\"Submission generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf6fc6-2365-4f0b-ae1a-8f924e37f19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c95dcf4c-f316-4a7a-8534-c9f354a5b7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "214c29b3-3cc2-47eb-8cd5-b3ba56aa036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('kaggel_nlp_getting_started-0125-04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5e9f1e2-390d-4b0d-bb0f-6a59f2517e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83879"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.83879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa479d4-281b-4298-a3a4-23e1d4658ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eee5ef1d-a1d0-44b5-893c-ed370a58d80d",
   "metadata": {},
   "source": [
    "<big>将 NLP 模型输出转化为“风控可用特征”  \n",
    "展示该特征如何用于下游风控（示意）<big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9fb6ae-5a55-43b1-9b6b-8ac5e783bbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 文本风险特征表（可接入风控系统）:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text_risk_score</th>\n",
       "      <th>is_high_risk_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>user_0</td>\n",
       "      <td>0.958550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>user_1</td>\n",
       "      <td>0.933660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>user_2</td>\n",
       "      <td>0.963134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>user_3</td>\n",
       "      <td>0.975063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>user_4</td>\n",
       "      <td>0.971232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id user_id  text_risk_score  is_high_risk_content\n",
       "0         0  user_0         0.958550                     1\n",
       "1         2  user_1         0.933660                     1\n",
       "2         3  user_2         0.963134                     1\n",
       "3         9  user_3         0.975063                     1\n",
       "4        11  user_4         0.971232                     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 账号主表 + 文本风险特征（风控模型输入示例）:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_count</th>\n",
       "      <th>login_days</th>\n",
       "      <th>text_risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.958550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.933660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.963134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_3</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.975063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_4</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>0.971232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  post_count  login_days  text_risk_score\n",
       "0  user_0           3           1         0.958550\n",
       "1  user_1           5           9         0.933660\n",
       "2  user_2           6          11         0.963134\n",
       "3  user_3           4           9         0.975063\n",
       "4  user_4           4          28         0.971232"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================\n",
    "# 新增：生成风控特征表\n",
    "# ==============================\n",
    "\n",
    "# 计算平均预测概率（soft voting）\n",
    "avg_logits = np.mean(all_test_logits, axis=0)  # shape: [n_test, 2]\n",
    "y_pred_proba = torch.softmax(torch.tensor(avg_logits), dim=1).numpy()  # 转为概率\n",
    "\n",
    "# 构造风控特征表\n",
    "risk_features = pd.DataFrame({\n",
    "    'tweet_id': df_test['id'].values,\n",
    "    'user_id': [f\"user_{i}\" for i in range(len(df_test))],  # 模拟用户ID\n",
    "    'text_risk_score': y_pred_proba[:, 1],  # 灾难类概率\n",
    "    'is_high_risk_content': (y_pred_proba[:, 1] > 0.7).astype(int)\n",
    "})\n",
    "\n",
    "print(\"\\n✅ 文本风险特征表（可接入风控系统）:\")\n",
    "display(risk_features.head())\n",
    "\n",
    "# （可选）模拟账号主表融合示例\n",
    "account_demo = pd.DataFrame({\n",
    "    'user_id': [f\"user_{i}\" for i in range(min(100, len(df_test)))],\n",
    "    'post_count': np.random.poisson(5, min(100, len(df_test))),\n",
    "    'login_days': np.random.randint(1, 30, min(100, len(df_test)))\n",
    "})\n",
    "merged = account_demo.merge(\n",
    "    risk_features[['user_id', 'text_risk_score']], \n",
    "    on='user_id', \n",
    "    how='left'\n",
    ").fillna(0)\n",
    "print(\"\\n✅ 账号主表 + 文本风险特征（风控模型输入示例）:\")\n",
    "display(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3375e-b027-4c25-9a97-a07bf2a56fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08df8e6a-d073-468b-be1a-be5f41ed0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注：在真实业务中，'user_id' 会来自用户行为日志，此处为演示模拟"
   ]
  },
  {
   "cell_type": "raw",
   "id": "965778c0-2a09-4096-ac4d-4b1b97121f16",
   "metadata": {},
   "source": [
    "### 📌 风控价值说明\n",
    "- 本模型输出的 `text_risk_score` 可作为**内容安全子模型**，为账号整体风险评分提供信号。\n",
    "- 在黑产使用**语义变形文本**（如谐音、符号替换）绕过关键词规则时，BERT 的语义理解能力可有效提升召回率。\n",
    "- 采用 PEFT 微调，训练成本低，适合风控场景**高频迭代、快速上线**的需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c271e41-805e-4de0-98ba-2e9bcc8889e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
