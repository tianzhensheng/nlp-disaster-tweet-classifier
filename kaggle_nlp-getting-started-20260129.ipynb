{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2770d-9268-4d40-8b5b-ca28776bea3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8311cf71-56b3-479d-a860-ac2ce698d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# åŸºç¡€è·¯å¾„\n",
    "base_path = Path(r\"C:\\Users\\ç”°\\Desktop\\pythonå®æ“\\kaggle\\Natural Language Processing with Disaster Tweets\")  # ä½ çš„åŸå§‹è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad225e38-9fba-4ee5-a711-aa6a255336c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(base_path / 'sample_submission.csv')\n",
    "test = pd.read_csv(base_path / 'test.csv')\n",
    "train = pd.read_csv(base_path / 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d630c99-3e4a-48a3-be54-da5bbedaa785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e80bfa6-a242-41ea-ae1f-0304f7719bcf",
   "metadata": {},
   "source": [
    "> **ç¯å¢ƒ**: Python 3.13, PyTorch 2.x, Transformers 4.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dab050-112a-4b35-bea2-9f27a249d4ce",
   "metadata": {},
   "source": [
    "<big>sft<big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5094f794-d83d-4cfa-9ed7-32d4d39c07ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\ç”°\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹ç¾éš¾æ¨æ–‡åˆ†ç±»è®­ç»ƒ\n",
      "è®¾å¤‡: cuda\n",
      "æ¨¡å‹: C:\\Users\\ç”°\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased\n",
      "\n",
      "ğŸ“Š åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...\n",
      "\n",
      "ğŸ”„ åŠ è½½tokenizerå’Œæ¨¡å‹\n",
      "âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n",
      "\n",
      "ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå…± 2 ä¸ªepoch\n",
      "\n",
      "==================================================\n",
      "Epoch 1/2\n",
      "==================================================\n",
      "  Batch 10/191, Loss: 0.6845\n",
      "  Batch 20/191, Loss: 0.6714\n",
      "  Batch 30/191, Loss: 0.6396\n",
      "  Batch 40/191, Loss: 0.6000\n",
      "  Batch 50/191, Loss: 0.5700\n",
      "  Batch 60/191, Loss: 0.5487\n",
      "  Batch 70/191, Loss: 0.5285\n",
      "  Batch 80/191, Loss: 0.5154\n",
      "  Batch 90/191, Loss: 0.5154\n",
      "  Batch 100/191, Loss: 0.5096\n",
      "  Batch 110/191, Loss: 0.5018\n",
      "  Batch 120/191, Loss: 0.4936\n",
      "  Batch 130/191, Loss: 0.4858\n",
      "  Batch 140/191, Loss: 0.4796\n",
      "  Batch 150/191, Loss: 0.4766\n",
      "  Batch 160/191, Loss: 0.4713\n",
      "  Batch 170/191, Loss: 0.4677\n",
      "  Batch 180/191, Loss: 0.4664\n",
      "  Batch 190/191, Loss: 0.4636\n",
      "è®­ç»ƒæŸå¤±: 0.4643\n",
      "éªŒè¯é›†F1åˆ†æ•°: 0.8393\n",
      "âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒF1: 0.8393\n",
      "\n",
      "==================================================\n",
      "Epoch 2/2\n",
      "==================================================\n",
      "  Batch 10/191, Loss: 0.3331\n",
      "  Batch 20/191, Loss: 0.3515\n",
      "  Batch 30/191, Loss: 0.3366\n",
      "  Batch 40/191, Loss: 0.3315\n",
      "  Batch 50/191, Loss: 0.3362\n",
      "  Batch 60/191, Loss: 0.3342\n",
      "  Batch 70/191, Loss: 0.3372\n",
      "  Batch 80/191, Loss: 0.3383\n",
      "  Batch 90/191, Loss: 0.3330\n",
      "  Batch 100/191, Loss: 0.3419\n",
      "  Batch 110/191, Loss: 0.3406\n",
      "  Batch 120/191, Loss: 0.3396\n",
      "  Batch 130/191, Loss: 0.3391\n",
      "  Batch 140/191, Loss: 0.3385\n",
      "  Batch 150/191, Loss: 0.3416\n",
      "  Batch 160/191, Loss: 0.3410\n",
      "  Batch 170/191, Loss: 0.3414\n",
      "  Batch 180/191, Loss: 0.3422\n",
      "  Batch 190/191, Loss: 0.3411\n",
      "è®­ç»ƒæŸå¤±: 0.3415\n",
      "éªŒè¯é›†F1åˆ†æ•°: 0.8461\n",
      "âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒF1: 0.8461\n",
      "\n",
      "ğŸ”® åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹...\n",
      "\n",
      "ğŸ’¾ ç”Ÿæˆæäº¤æ–‡ä»¶...\n",
      "âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: bert_sft_submission.csv\n",
      "ğŸ“ˆ æœ€ä½³F1åˆ†æ•°: 0.8461\n",
      "\n",
      "âœ… è®­ç»ƒå®Œæˆ!\n",
      "\n",
      "ğŸ” éªŒè¯é¢„æµ‹ç»“æœ:\n",
      "æµ‹è¯•é›†å¤§å°: 3263\n",
      "é¢„æµ‹åˆ†å¸ƒ: 0=1989, 1=1274\n",
      "CPU times: total: 3min 36s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ==============================\n",
    "# ç¾éš¾æ¨æ–‡åˆ†ç±» - SFTé£æ ¼BERT\n",
    "# ä¿æŒBERTæ¨¡å‹ï¼Œä½†ç”¨ç”Ÿæˆå¼è®­ç»ƒæ–¹å¼\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,  # ç”¨å›åˆ†ç±»æ¨¡å‹\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================\n",
    "# 1. é…ç½®å‚æ•°\n",
    "# ==============================\n",
    "SEED = 42\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LR = 2e-5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ä½¿ç”¨BERTæ¨¡å‹\n",
    "MODEL_NAME = r'C:\\Users\\ç”°\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased'\n",
    "\n",
    "# ==============================\n",
    "# 2. æ•°æ®å¤„ç†å‡½æ•°\n",
    "# ==============================\n",
    "def clean_text(tweet):\n",
    "    \"\"\"æ¸…ç†æ–‡æœ¬\"\"\"\n",
    "    tweet = str(tweet)\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "def format_sft_prompt(text, label=None):\n",
    "    \"\"\"\n",
    "    æ ¼å¼åŒ–æŒ‡ä»¤æç¤º - ä¸ºBERTè°ƒæ•´\n",
    "    \"\"\"\n",
    "    instruction = \"åˆ¤æ–­ä»¥ä¸‹æ¨æ–‡æ˜¯å¦æè¿°çœŸå®ç¾éš¾äº‹ä»¶ï¼š\"\n",
    "    prompt = f\"{instruction} {text}\"\n",
    "    return prompt\n",
    "\n",
    "# ==============================\n",
    "# 3. æ•°æ®é›†ç±»\n",
    "# ==============================\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = format_sft_prompt(str(self.texts[idx]))\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        else:\n",
    "            # æµ‹è¯•æ•°æ®æ²¡æœ‰æ ‡ç­¾\n",
    "            item['labels'] = torch.tensor(0, dtype=torch.long)  # å ä½ç¬¦\n",
    "            \n",
    "        return item\n",
    "\n",
    "# ==============================\n",
    "# 4. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°\n",
    "# ==============================\n",
    "def train_epoch(model, dataloader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            print(f\"  Batch {batch_idx+1}/{len(dataloader)}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "    return f1, all_predictions\n",
    "\n",
    "# ==============================\n",
    "# 5. ä¸»è®­ç»ƒæµç¨‹\n",
    "# ==============================\n",
    "def main():\n",
    "    print(\"ğŸš€ å¼€å§‹ç¾éš¾æ¨æ–‡åˆ†ç±»è®­ç»ƒ\")\n",
    "    print(f\"è®¾å¤‡: {DEVICE}\")\n",
    "    print(f\"æ¨¡å‹: {MODEL_NAME}\")\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    print(\"\\nğŸ“Š åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...\")\n",
    "    df_train = train\n",
    "    df_test = test\n",
    "    \n",
    "    # æ¸…ç†æ–‡æœ¬\n",
    "    df_train['text_cleaned'] = df_train['text'].apply(clean_text)\n",
    "    df_test['text_cleaned'] = df_test['text'].apply(clean_text)\n",
    "    \n",
    "    # åˆå§‹åŒ–tokenizerå’Œæ¨¡å‹\n",
    "    print(f\"\\nğŸ”„ åŠ è½½tokenizerå’Œæ¨¡å‹\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # å…³é”®ä¿®å¤ï¼šä¸ºBERTè®¾ç½®padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df_train['text_cleaned'].values,\n",
    "        df_train['target'].values,\n",
    "        test_size=0.2,\n",
    "        random_state=SEED,\n",
    "        stratify=df_train['target']\n",
    "    )\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®é›†\n",
    "    train_dataset = TweetDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = TweetDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "    test_dataset = TweetDataset(df_test['text_cleaned'].values, None, tokenizer, MAX_LEN)\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # ä¼˜åŒ–å™¨\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(total_steps * 0.1),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒå¾ªç¯\n",
    "    print(f\"\\nğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå…± {EPOCHS} ä¸ªepoch\")\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # è®­ç»ƒ\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "        print(f\"è®­ç»ƒæŸå¤±: {train_loss:.4f}\")\n",
    "        \n",
    "        # è¯„ä¼°\n",
    "        f1_score_val, _ = evaluate_model(model, val_loader)\n",
    "        print(f\"éªŒè¯é›†F1åˆ†æ•°: {f1_score_val:.4f}\")\n",
    "        \n",
    "        if f1_score_val > best_f1:\n",
    "            best_f1 = f1_score_val\n",
    "            torch.save(model.state_dict(), f\"best_model_epoch{epoch+1}.pth\")\n",
    "            print(f\"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒF1: {best_f1:.4f}\")\n",
    "    \n",
    "    # æµ‹è¯•é›†é¢„æµ‹\n",
    "    print(f\"\\nğŸ”® åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹...\")\n",
    "    if EPOCHS > 0:\n",
    "        model.load_state_dict(torch.load(f\"best_model_epoch{EPOCHS}.pth\"))\n",
    "    \n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_predictions.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    # ç”Ÿæˆæäº¤æ–‡ä»¶\n",
    "    print(f\"\\nğŸ’¾ ç”Ÿæˆæäº¤æ–‡ä»¶...\")\n",
    "    submission = sample_submission.copy()\n",
    "    submission['target'] = test_predictions[:len(submission)]\n",
    "    \n",
    "    submission.to_csv('bert_sft_submission.csv', index=False)\n",
    "    print(f\"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: bert_sft_submission.csv\")\n",
    "    print(f\"ğŸ“ˆ æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# ==============================\n",
    "# 6. è¿è¡Œè®­ç»ƒ\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        submission = main()\n",
    "        print(\"\\nâœ… è®­ç»ƒå®Œæˆ!\")\n",
    "        \n",
    "        # éªŒè¯å‰å‡ ä¸ªé¢„æµ‹\n",
    "        print(\"\\nğŸ” éªŒè¯é¢„æµ‹ç»“æœ:\")\n",
    "        print(f\"æµ‹è¯•é›†å¤§å°: {len(submission)}\")\n",
    "        print(f\"é¢„æµ‹åˆ†å¸ƒ: 0={sum(submission['target']==0)}, 1={sum(submission['target']==1)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ é”™è¯¯: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e087693-7bf4-4119-99d1-d5fe6717c421",
   "metadata": {},
   "source": [
    "Score: 0.83205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3375e-b027-4c25-9a97-a07bf2a56fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08df8e6a-d073-468b-be1a-be5f41ed0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ³¨ï¼šåœ¨çœŸå®ä¸šåŠ¡ä¸­ï¼Œ'user_id' ä¼šæ¥è‡ªç”¨æˆ·è¡Œä¸ºæ—¥å¿—ï¼Œæ­¤å¤„ä¸ºæ¼”ç¤ºæ¨¡æ‹Ÿ"
   ]
  },
  {
   "cell_type": "raw",
   "id": "965778c0-2a09-4096-ac4d-4b1b97121f16",
   "metadata": {},
   "source": [
    "### ğŸ“Œ é£æ§ä»·å€¼è¯´æ˜\n",
    "- æœ¬æ¨¡å‹è¾“å‡ºçš„ `text_risk_score` å¯ä½œä¸º**å†…å®¹å®‰å…¨å­æ¨¡å‹**ï¼Œä¸ºè´¦å·æ•´ä½“é£é™©è¯„åˆ†æä¾›ä¿¡å·ã€‚\n",
    "- åœ¨é»‘äº§ä½¿ç”¨**è¯­ä¹‰å˜å½¢æ–‡æœ¬**ï¼ˆå¦‚è°éŸ³ã€ç¬¦å·æ›¿æ¢ï¼‰ç»•è¿‡å…³é”®è¯è§„åˆ™æ—¶ï¼ŒBERT çš„è¯­ä¹‰ç†è§£èƒ½åŠ›å¯æœ‰æ•ˆæå‡å¬å›ç‡ã€‚\n",
    "- é‡‡ç”¨ PEFT å¾®è°ƒï¼Œè®­ç»ƒæˆæœ¬ä½ï¼Œé€‚åˆé£æ§åœºæ™¯**é«˜é¢‘è¿­ä»£ã€å¿«é€Ÿä¸Šçº¿**çš„éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c271e41-805e-4de0-98ba-2e9bcc8889e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
