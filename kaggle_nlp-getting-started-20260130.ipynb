{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2770d-9268-4d40-8b5b-ca28776bea3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8311cf71-56b3-479d-a860-ac2ce698d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# åŸºç¡€è·¯å¾„\n",
    "base_path = Path(r\"C:\\Users\\ç”°\\Desktop\\pythonå®æ“\\kaggle\\Natural Language Processing with Disaster Tweets\")  # ä½ çš„åŸå§‹è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad225e38-9fba-4ee5-a711-aa6a255336c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(base_path / 'sample_submission.csv')\n",
    "test = pd.read_csv(base_path / 'test.csv')\n",
    "train = pd.read_csv(base_path / 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d630c99-3e4a-48a3-be54-da5bbedaa785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e80bfa6-a242-41ea-ae1f-0304f7719bcf",
   "metadata": {},
   "source": [
    "> **ç¯å¢ƒ**: Python 3.13, PyTorch 2.x, Transformers 4.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dab050-112a-4b35-bea2-9f27a249d4ce",
   "metadata": {},
   "source": [
    "<big>sft<big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5094f794-d83d-4cfa-9ed7-32d4d39c07ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨ç¾éš¾æ¨æ–‡åˆ†ç±»è®­ç»ƒæµç¨‹\n",
      "è®¾å¤‡: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0130 22:21:34.084000 4512 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\ç”°\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Epoch 1/2\n",
      "========================================\n",
      "è®­ç»ƒæŸå¤±: 0.4858 | éªŒè¯F1: 0.8371\n",
      "âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ (F1=0.8371)\n",
      "\n",
      "========================================\n",
      "Epoch 2/2\n",
      "========================================\n",
      "è®­ç»ƒæŸå¤±: 0.3538 | éªŒè¯F1: 0.8496\n",
      "âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ (F1=0.8496)\n",
      "\n",
      "ğŸ”® ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...\n",
      "âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜ï¼Œæœ€ä½³éªŒè¯F1: 0.8496\n",
      "\n",
      "ğŸ“Š é¢„æµ‹åˆ†å¸ƒ: éç¾éš¾=2016, ç¾éš¾=1247\n",
      "CPU times: total: 3min 49s\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================\n",
    "# é…ç½®å‚æ•°\n",
    "# ==============================\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'max_len': 128,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 2,\n",
    "    'learning_rate': 2e-5,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'model_path': r'C:\\Users\\ç”°\\.cache\\modelscope\\hub\\models\\google-bert\\bert-base-uncased'\n",
    "}\n",
    "\n",
    "# ==============================\n",
    "# æ–‡æœ¬é¢„å¤„ç†\n",
    "# ==============================\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"æ¸…ç†æ¨æ–‡æ–‡æœ¬ï¼šç§»é™¤URLã€@æåŠã€#æ ‡ç­¾ï¼Œå¹¶è§„èŒƒåŒ–ç©ºæ ¼\"\"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)      # ç§»é™¤é“¾æ¥\n",
    "    text = re.sub(r'@\\w+', '', text)                       # ç§»é™¤@ç”¨æˆ·\n",
    "    text = re.sub(r'#', '', text)                          # ç§»é™¤äº•å·ï¼ˆä¿ç•™è¯ï¼‰\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()               # åˆå¹¶å¤šä½™ç©ºæ ¼\n",
    "    return text\n",
    "\n",
    "def build_prompt(text: str) -> str:\n",
    "    \"\"\"æ„å»ºåˆ†ç±»ä»»åŠ¡æç¤º\"\"\"\n",
    "    return f\"åˆ¤æ–­ä»¥ä¸‹æ¨æ–‡æ˜¯å¦æè¿°çœŸå®ç¾éš¾äº‹ä»¶ï¼š{text}\"\n",
    "\n",
    "# ==============================\n",
    "# è‡ªå®šä¹‰æ•°æ®é›†\n",
    "# ==============================\n",
    "class DisasterTweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = build_prompt(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "        }\n",
    "\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        else:\n",
    "            item['labels'] = torch.tensor(0, dtype=torch.long)  # æµ‹è¯•é›†å ä½\n",
    "\n",
    "        return item\n",
    "\n",
    "# ==============================\n",
    "# è®­ç»ƒä¸è¯„ä¼°\n",
    "# ==============================\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return f1, all_preds\n",
    "\n",
    "# ==============================\n",
    "# ä¸»æµç¨‹\n",
    "# ==============================\n",
    "def main(train_df, test_df, sample_sub):\n",
    "    print(\"ğŸš€ å¯åŠ¨ç¾éš¾æ¨æ–‡åˆ†ç±»è®­ç»ƒæµç¨‹\")\n",
    "    print(f\"è®¾å¤‡: {CONFIG['device']}\")\n",
    "    \n",
    "    # æ•°æ®æ¸…æ´—\n",
    "    train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
    "    test_df['clean_text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_df['clean_text'].values,\n",
    "        train_df['target'].values,\n",
    "        test_size=0.2,\n",
    "        random_state=CONFIG['seed'],\n",
    "        stratify=train_df['target']\n",
    "    )\n",
    "\n",
    "    # åˆå§‹åŒ– tokenizer å’Œæ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_path'])\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CONFIG['model_path'], num_labels=2\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(CONFIG['device'])\n",
    "\n",
    "    # æ„å»ºæ•°æ®åŠ è½½å™¨\n",
    "    train_dataset = DisasterTweetDataset(X_train, y_train, tokenizer, CONFIG['max_len'])\n",
    "    val_dataset = DisasterTweetDataset(X_val, y_val, tokenizer, CONFIG['max_len'])\n",
    "    test_dataset = DisasterTweetDataset(test_df['clean_text'].values, None, tokenizer, CONFIG['max_len'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "    # ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(total_steps * 0.1),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # è®­ç»ƒå¾ªç¯\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        print(f\"\\n{'='*40}\\nEpoch {epoch + 1}/{CONFIG['epochs']}\\n{'='*40}\")\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, CONFIG['device'])\n",
    "        val_f1, _ = evaluate(model, val_loader, CONFIG['device'])\n",
    "\n",
    "        print(f\"è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), f\"best_model_epoch{epoch+1}.pth\")\n",
    "            print(f\"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ (F1={best_f1:.4f})\")\n",
    "\n",
    "    # æµ‹è¯•é›†é¢„æµ‹\n",
    "    print(\"\\nğŸ”® ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...\")\n",
    "    model.load_state_dict(torch.load(f\"best_model_epoch{CONFIG['epochs']}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(CONFIG['device'])\n",
    "            attention_mask = batch['attention_mask'].to(CONFIG['device'])\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # ä¿å­˜æäº¤æ–‡ä»¶\n",
    "    submission = sample_sub.copy()\n",
    "    submission['target'] = predictions[:len(submission)]\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(f\"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜ï¼Œæœ€ä½³éªŒè¯F1: {best_f1:.4f}\")\n",
    "\n",
    "    return submission\n",
    "\n",
    "# ==============================\n",
    "# å…¥å£ç‚¹ï¼ˆå‡è®¾ train/test/sample_submission å·²å®šä¹‰ï¼‰\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # æ³¨æ„ï¼šæ­¤å¤„éœ€ç¡®ä¿ train, test, sample_submission å·²åœ¨å…¨å±€ä½œç”¨åŸŸä¸­å®šä¹‰\n",
    "        # ï¼ˆä¾‹å¦‚ä» Kaggle ç¯å¢ƒæˆ–æœ¬åœ° CSV åŠ è½½ï¼‰\n",
    "        submission = main(train, test, sample_submission)\n",
    "\n",
    "        # è¾“å‡ºé¢„æµ‹ç»Ÿè®¡\n",
    "        print(f\"\\nğŸ“Š é¢„æµ‹åˆ†å¸ƒ: éç¾éš¾={sum(submission['target']==0)}, ç¾éš¾={sum(submission['target']==1)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e087693-7bf4-4119-99d1-d5fe6717c421",
   "metadata": {},
   "source": [
    "score:0.84462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3375e-b027-4c25-9a97-a07bf2a56fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "965778c0-2a09-4096-ac4d-4b1b97121f16",
   "metadata": {},
   "source": [
    "é£æ§åº”ç”¨ä»·å€¼\n",
    "æ¨¡å‹è¾“å‡ºçš„ text_risk_score å¯ä½œä¸ºå†…å®¹å®‰å…¨å­æ¨¡å—ï¼Œèå…¥è´¦å·æ•´ä½“é£é™©è¯„ä¼°ä½“ç³»ã€‚\n",
    "ç›¸æ¯”å…³é”®è¯è§„åˆ™ï¼ŒåŸºäºè¯­ä¹‰çš„æ¨¡å‹èƒ½æœ‰æ•ˆè¯†åˆ«è°éŸ³ã€ç¬¦å·æ›¿æ¢ç­‰å˜å½¢æ–‡æœ¬ï¼Œæå‡é»‘äº§å†…å®¹çš„å¬å›èƒ½åŠ›ã€‚\n",
    "é‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ï¼Œè®­ç»ƒèµ„æºæ¶ˆè€—ä½ï¼Œæ”¯æŒé£æ§ç­–ç•¥å¿«é€Ÿè¿­ä»£ä¸ä¸Šçº¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c271e41-805e-4de0-98ba-2e9bcc8889e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
